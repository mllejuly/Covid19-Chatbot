{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT based COVID-19 Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sentence-transformers pre-trained models to local\n",
    "# https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/\n",
    "\n",
    "# choose model\n",
    "base_model = 'bert-base-nli-mean-tokens'\n",
    "model = SentenceTransformer('./model/' + base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data path\n",
    "train_data_path = \"data/training_data.csv\"\n",
    "eval_data_path = \"data/evaluation_data.csv\"\n",
    "\n",
    "# set parameters\n",
    "\n",
    "# epochs: [1,2,3,5,10,15,20]\n",
    "epochs=3\n",
    "\n",
    "# batch_size: [32,64,128]\n",
    "batch_size = 16\n",
    "\n",
    "# learning_rate: [2e-01, 2e-03, 2e-05, 2e-07]\n",
    "learning_rate = 2e-05\n",
    "\n",
    "\n",
    "# fixed\n",
    "optimizer_eps = 2e-05\n",
    "warmup_steps=100\n",
    "evaluation_steps=500\n",
    "\n",
    "\n",
    "# save model\n",
    "new_model = base_model + '-eps' + str(epochs) + '-batch' + str(batch_size) + '-lr' + str(learning_rate)\n",
    "model_save_path = 'model/' + new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Data\n",
    "To represent our training data, we use the InputExample class to store training examples. As parameters, it accepts texts, which is a list of strings representing our pairs (or triplets). Further, we can also pass a label (either float or int). The following shows a simple example, where we pass text pairs to InputExample together with a label indicating the semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_data_path).loc[:, ['question1', 'question2', 'is_duplicate']].dropna()\n",
    "train = train_df.apply(lambda row: InputExample(texts=[row[0], row[1]], label=float(row[2])), axis=1)\n",
    "\n",
    "train_dataset = SentencesDataset(train, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "The loss function plays a critical role when fine-tuning the model. It determines how well our embedding model will work for the specific downstream task.\n",
    "\n",
    "Sadly there is no “one size fits all” loss function. Which loss function is suitable depends on the available training data and on the target task.\n",
    "\n",
    "To fine-tune our network, we need somehow to tell our network which sentence pairs are similar, and should be close in vector space, and which pairs are dissimilar, and should be far away in vector space.\n",
    "\n",
    "The most simple way is to have sentence pairs annotated with a score indicating their similarity, e.g. on a scale 0 to 1. We can then train the network with a Siamese Network Architecture.\n",
    "\n",
    "For each sentence pair, we pass sentence A and sentence B through our network which yields the embeddings u und v. The similarity of these embeddings is computed using cosine similarity and the result is compared to the gold similarity score. This allows our network to be fine-tuned and to recognize the similarity of sentences.\n",
    "\n",
    "We tune the model by calling model.fit(). We pass a list of train_objectives, which constist of tuples (dataloader, loss_function). We can pass more than one tuple in order to perform multi-task learning on several datasets with different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.CosineSimilarityLoss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluators\n",
    "During training, we usually want to measure the performance to see if the performance improves. For this, the sentence_transformers.evaluation package exists. It contains various evaluators which we can pass to the fit-method. These evaluators are run periodically during training. Further, they return a score and only the model with the highest score will be stored on disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(eval_data_path).loc[:, ['question1', 'question2', 'is_duplicate']].dropna()\n",
    "sentences1 = eval_df['question1'].tolist()\n",
    "sentences2 = eval_df['question2'].tolist()\n",
    "scores = [float(i) for i in eval_df['is_duplicate'].tolist()]\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "          epochs=epochs,\n",
    "          optimizer_params={'correct_bias': False, \n",
    "                            'eps': optimizer_eps,\n",
    "                            'lr': learning_rate},\n",
    "          warmup_steps=warmup_steps,\n",
    "          evaluator=evaluator,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          # save model\n",
    "          output_path=model_save_path\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
